{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:29:47.907650Z",
     "start_time": "2024-12-03T03:29:47.245714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data = pd.read_excel('cleaned_dataset.xlsx')\n",
    "y = data['Class']\n",
    "transport_columns = [col for col in data.columns if 'Type_of_Transport_Used_' in col]\n",
    "transport_data = data[transport_columns]\n",
    "transport = transport_data.astype(int).to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(transport, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(transport, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 2: Initialize parameters\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y))  # 4 obesity levels\n",
    "W = np.random.randn(n_features, n_classes) * 0.01  # Small random weights\n",
    "b = np.zeros((1, n_classes))  # Bias initialized to zero\n",
    "\n",
    "# Step 3: Define helper functions\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability trick\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def compute_gradients(X, y_true, y_prob, y_pred):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Gradients for weights and bias (based on probabilities)\n",
    "    dW = (1 / n_samples) * np.dot(X.T, (y_prob - np.eye(n_classes)[y_true.astype(int) - 1]))\n",
    "    db = (1 / n_samples) * np.sum(y_prob - np.eye(n_classes)[y_true.astype(int) - 1], axis=0, keepdims=True)\n",
    "    \n",
    "    return dW, db\n",
    "\n",
    "# Step 4: Training loop\n",
    "learning_rate = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    logits = np.dot(X_train, W) + b  # Compute logits\n",
    "    y_prob = softmax(logits)  # Class probabilities\n",
    "    y_pred = np.sum(y_prob * np.arange(1, n_classes + 1), axis=1)  # Expected value (scalar prediction)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = mean_squared_error(y_train, y_pred)\n",
    "    \n",
    "    # Backward pass\n",
    "    dW, db = compute_gradients(X_train, y_train, y_prob, y_pred)\n",
    "    \n",
    "    # Update parameters\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Step 5: Evaluate on test set\n",
    "logits_test = np.dot(X_test, W) + b\n",
    "y_prob_test = softmax(logits_test)\n",
    "y_pred_test = np.sum(y_prob_test * np.arange(1, n_classes + 1), axis=1)\n",
    "test_loss = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "y_pred_test_rounded = np.round(y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test_rounded, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6925\n",
      "Epoch 100, Loss: 0.6668\n",
      "Epoch 200, Loss: 0.6635\n",
      "Epoch 300, Loss: 0.6626\n",
      "Epoch 400, Loss: 0.6624\n",
      "Epoch 500, Loss: 0.6623\n",
      "Epoch 600, Loss: 0.6623\n",
      "Epoch 700, Loss: 0.6623\n",
      "Epoch 800, Loss: 0.6623\n",
      "Epoch 900, Loss: 0.6623\n",
      "Test Loss: 0.6770\n",
      "F1 Score: 0.2201\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T03:35:21.702180Z",
     "start_time": "2024-12-03T03:35:21.693928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define a function to train and evaluate the Softmax Regression\n",
    "def train_softmax(X_train, y_train, X_val, y_val, learning_rate, lambda_reg, batch_size, n_epochs=1000):\n",
    "    n_samples, n_features = X_train.shape\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    W = np.random.randn(n_features, n_classes) * 0.01\n",
    "    b = np.zeros((1, n_classes))\n",
    "\n",
    "    # Convert y_train and y_val to one-hot encoding\n",
    "    y_train_onehot = np.eye(n_classes)[y_train]\n",
    "    y_val_onehot = np.eye(n_classes)[y_val]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Mini-batch gradient descent\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train_onehot[i:i + batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            logits = np.dot(X_batch, W) + b\n",
    "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Stability trick\n",
    "            probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "            \n",
    "            # Compute loss (cross-entropy with L2 regularization)\n",
    "            loss = -np.mean(np.sum(y_batch * np.log(probs + 1e-9), axis=1)) + (lambda_reg / 2) * np.sum(W ** 2)\n",
    "            \n",
    "            # Gradients\n",
    "            dW = np.dot(X_batch.T, (probs - y_batch)) / batch_size + lambda_reg * W\n",
    "            db = np.sum(probs - y_batch, axis=0, keepdims=True) / batch_size\n",
    "            \n",
    "            # Parameter updates\n",
    "            W -= learning_rate * dW\n",
    "            b -= learning_rate * db\n",
    "\n",
    "    # Validation f1 score\n",
    "    logits_val = np.dot(X_val, W) + b\n",
    "    probs_val = np.exp(logits_val - np.max(logits_val, axis=1, keepdims=True))\n",
    "    probs_val /= np.sum(probs_val, axis=1, keepdims=True)\n",
    "    y_pred = np.argmax(probs_val, axis=1)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    return f1, loss\n"
   ],
   "id": "941036deca7bc4d8",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define cross-validation function\n",
    "def cross_validate_softmax(X, y, learning_rate, lambda_reg, batch_size, n_folds=5, n_epochs=500):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Split the data into training and validation sets\n",
    "        X_array = np.array(X)\n",
    "        y_array = np.array(y)\n",
    "        X_train, X_val = X_array[train_idx], X_array[val_idx]\n",
    "        y_train, y_val = y_array[train_idx], y_array[val_idx]\n",
    "\n",
    "        # Train the model and evaluate f1 score\n",
    "        f1, _ = train_softmax(X_train, y_train, X_val, y_val, learning_rate, lambda_reg, batch_size, n_epochs)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Return the average f1 score across folds\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "lambda_regs = [1e-5, 1e-4, 1e-3]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_f1 = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "# Perform cross-validation for each combination of hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for reg in lambda_regs:\n",
    "        for batch in batch_sizes:\n",
    "            f1 = cross_validate_softmax(X_train, y_train, lr, reg, batch)\n",
    "            print(f\"LR: {lr}, Lambda: {reg}, Batch Size: {batch}, Cross-Validated f1 score: {f1:.4f}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_hyperparams = {'learning_rate': lr, 'lambda_reg': reg, 'batch_size': batch}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "print(\"Best Cross-Validated f1 score:\", best_f1)"
   ],
   "id": "26abdaf5d830455e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import optuna\n",
    "# Objective function for hyperparameter optimization\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1)\n",
    "    lambda_reg = trial.suggest_loguniform(\"lambda_reg\", 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    accuracy, _ = train_softmax(X_train, y_train, X_val, y_val, learning_rate, lambda_reg, batch_size)\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best hyperparameters and result\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best Validation Accuracy:\", study.best_value)\n"
   ],
   "id": "abadc018e020dfce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
